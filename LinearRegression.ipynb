{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8cc4b2",
   "metadata": {},
   "source": [
    "# Gradient Descent Visualizer for Multi-Linear Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we hope to create a visualizer for gradient descent with multilinear regression. In order to complete this project, we must:\n",
    "\n",
    "    - Obtain a dataset. In order to make using the tool a simple experience, we will simply generate a dataset \n",
    "    based on how many features the user wishes to visualize. There will be 100 training examples by default,\n",
    "    however, the user may choose how many training examples they wish to go through\n",
    "    \n",
    "    - Gather values for our parameters in order to use them for our equations (such as the constant bias, \n",
    "    factor (explained later), and the variability of our factor\n",
    "    \n",
    "    - Create various graphs for the user and animate them. If the user wishes to watch a single step, \n",
    "    they may pause the animation and go step by step. The graphs shown will be the cost function graph, the linear regression plot, the topological cost graph, and the individual graph for each variable with the other \n",
    "    variable remaining constant (more on that later).\n",
    "    \n",
    "When plotting our current $w$ and $b$ values on the cost function graph, we will display the current point as a red ball of sorts, in order to display how gradient descent works, similar to a ball rolling down a hill.\n",
    "\n",
    "Our goal is to allow anyone to easily visualize how gradient descent is carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89515499",
   "metadata": {},
   "source": [
    "## Generating a dataset\n",
    "\n",
    "We must begin by generating a dataset to perform gradient descent on. Given the amount of features the user desires, we will generate a dataset with that many inputs. In order to generate our inputs, we will generate random numbers between a given range by the user (or a randomly generated range if no user input is given). We will also generate random values for $\\vec{w}$, a vector with length $j$, which is the number of features in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a9fab",
   "metadata": {},
   "source": [
    "Once we generate our factors, we will create a $i\\times j$ matrix, where $i$ is the number of training examples, and $j$ is the number of features. When going through each feature in a specific training example, we will generate the random x values and place them in our dataset. Additionally, we will generate a constant bias between 0 and 100 if no user input for it is given. For our outputs, $y=\\vec{w} \\cdot \\vec{x} + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f83d213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sympy as smp\n",
    "import random as rd\n",
    "\n",
    "def generate_current_vector_w (vector_w, variability):\n",
    "    current_vector_w = []\n",
    "    for w in vector_w:\n",
    "            mu, sigma = w, w * variability\n",
    "            new_factor = np.random.normal(mu, sigma, 1)\n",
    "            current_vector_w.append(new_factor)\n",
    "    return current_vector_w\n",
    "\n",
    "def generate_dataset (number_training_examples, number_features, variability, bias):\n",
    "    ## We begin by generating a range for us to generate random x-values with, as well as generating a vector for\n",
    "    ## all of our w values, and by generating an empty dataset\n",
    "    \n",
    "    ranges = np.round(np.random.uniform(1, 10000, number_features))\n",
    "    vector_w = np.round(np.random.uniform(0, 100, number_features), 1)\n",
    "    dataset = np.empty((number_training_examples, number_features))\n",
    "    \n",
    "    ## We now generate the values for our training examples and assign them into our dataset\n",
    "    for i in range (number_training_examples):\n",
    "        for j in range (number_features):\n",
    "            dataset[i][j] = rd.uniform(0, ranges[j])\n",
    "        \n",
    "    ## We now create our vector for our outputs\n",
    "    vector_y = []\n",
    "    \n",
    "    for training_example in dataset:\n",
    "        current_vector_w = generate_current_vector_w (vector_w, variability)\n",
    "        y = np.dot(training_example, current_vector_w) + bias\n",
    "        vector_y.append(y)\n",
    "        \n",
    "    vector_y = np.array(vector_y)\n",
    "        \n",
    "    return dataset, vector_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f0597",
   "metadata": {},
   "source": [
    "## Regression Line and Cost Function\n",
    "\n",
    "We must now create our algorithm for our cost function. Our linear regression line equation is as follows: <h3><center>$$f_{\\vec{w},b}(\\vec{x})=\\vec{w}\\cdot\\vec{x} + b$$</center></h3>\n",
    "\n",
    "For this cost function, we will use the squared-error cost function. This function is as follows, where $i$ is the current training example, and $m$ is the total number of training examples: <h1><center>$$J(\\vec{w},b)=\\frac{1}{2m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})^2$$</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63afeee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_line (w, x, b):\n",
    "    \"\"\"\n",
    "    Returns our predicted value of y\n",
    "    \"\"\"\n",
    "    return np.dot(w, x) + b\n",
    "\n",
    "def cost_function(w, b, dataset, output):\n",
    "    \"\"\"\n",
    "    Calculates the cost of our current w and b\n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    shape = dataset.shape\n",
    "    rows = shape[0]\n",
    "    for i in range (rows):\n",
    "        cost = cost + (linear_regression_line(w, dataset[i], b) - output[i]) ** 2\n",
    "    return cost/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068f158",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We must now create our algorithm for gradient descent in order to find optimal values for $\\vec{w}$ and $b$. We will first begin with our gradient descent, which is as follows, where $\\alpha$ is our learning rate, and $w_{j}$ is equal to the $j^{th}$ feature's $w$ value: \n",
    "\n",
    "<h1><center>$$w_{j}=w_{j}-\\alpha\\frac{\\partial}{\\partial w_j}J(\\vec{w},b)$$</center></h1> \n",
    "\n",
    "<h1><center>$$b=b-\\alpha\\frac{\\partial}{\\partial b}J(\\vec{w},b)$$</center></h1> \n",
    "\n",
    "Once we expand our equation by substituting our cost function into the formulas above, we get:\n",
    "\n",
    "<h1><center>$$w_{j}=w_{j}-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_j^{(i)}$$</center></h1> \n",
    "\n",
    "<h1><center>$$b=b-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})$$</center></h1>\n",
    "\n",
    "We are now able to program this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent (vector_w, bias, dataset, output):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
